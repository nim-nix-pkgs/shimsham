import blake2s
import unsigned

proc blocks*(d: var Blake2s, p: openarray[byte]) =
  var (h0,h1,h2,h3,h4,h5,h6,h7) = (d.h[0],d.h[1],d.h[2],d.h[3],d.h[4],d.h[5],d.h[6],d.h[7])
  var p = @p

  while p.len >= BlockSize:
    # increment counter
    d.t[0] += BlockSize
    if d.t[0] < BlockSize:
      inc d.t[1]
    # initialize compression function
    var (v0,v1,v2,v3,v4,v5,v6,v7) = (h0,h1,h2,h3,h4,h5,h6,h7)
    var v8 = IV[0]
    var v9 = IV[1]
    var v10 = IV[2]
    var v11 = IV[3]
    var v12 = IV[4] xor d.t[0]
    var v13 = IV[5] xor d.t[1]
    var v14 = IV[6] xor d.f[0]
    var v15 = IV[7] xor d.f[1]

    var m: array[16, uint32]

    for i in 0..<4:
      m[i] = p[i*4+0].uint32 or (p[i*4+1].uint32 shl 8) or 
        (p[i*4+2].uint32 shl 16) or (p[i*4+3].uint32 shl 24)

    #echo "m: ", (@m)

    # Round 1.
    block rnd1:
      v0 += m[0]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v1 += m[2]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-16)) or ((v13 shr 16))
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v2 += m[4]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v3 += m[6]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v2 += m[5]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v3 += m[7]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v1 += m[3]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-7)) or (v5 shr 7)
      v0 += m[1]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v0 += m[8]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v1 += m[10]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v2 += m[12]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v3 += m[14]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v2 += m[13]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v3 += m[15]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v1 += m[11]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v0 += m[9]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-7)) or (v5 shr 7)

    # Round 2.
    block rnd2:
      v0 += m[14]
      v0 += v4
      v12 = v12 xor 0
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v8 += v12
      v4 = v4 xor 8
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v1 += m[4]
      v1 += v5
      v13 = v13 xor 1
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v9 += v13
      v5 = v5 xor 9
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v2 += m[9]
      v2 += v6
      v14 = v14 xor 2
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v10 += v14
      v6 = v6 xor 10
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v3 += m[13]
      v3 += v7
      v15 = v15 xor 3
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v11 += v15
      v7 = v7 xor 11
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v2 += m[15]
      v2 += v6
      v14 = v14 xor 2
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v10 += v14
      v6 = v6 xor 10
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v3 += m[6]
      v3 += v7
      v15 = v15 xor 3
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v11 += v15
      v7 = v7 xor 11
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v1 += m[8]
      v1 += v5
      v13 = v13 xor 1
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v9 += v13
      v5 = v5 xor 9
      v5 = (v5 shl (32-7)) or (v5 shr 7)
      v0 += m[10]
      v0 += v4
      v12 = v12 xor 0
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v8 += v12
      v4 = v4 xor 8
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v0 += m[1]
      v0 += v5
      v15 = v15 xor 0
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v10 += v15
      v5 = v5 xor 10
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v1 += m[0]
      v1 += v6
      v12 = v12 xor 1
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v11 += v12
      v6 = v6 xor 11
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v2 += m[11]
      v2 += v7
      v13 = v13 xor 2
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v8 += v13
      v7 = v7 xor 8
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v3 += m[5]
      v3 += v4
      v14 = v14 xor 3
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v9 += v14
      v4 = v4 xor 9
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v2 += m[7]
      v2 += v7
      v13 = v13 xor 2
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v8 += v13
      v7 = v7 xor 8
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v3 += m[3]
      v3 += v4
      v14 = v14 xor 3
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v9 += v14
      v4 = v4 xor 9
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v1 += m[2]
      v1 += v6
      v12 = v12 xor 1
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v11 += v12
      v6 = v6 xor 11
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v0 += m[12]
      v0 += v5
      v15 = v15 xor 0
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v10 += v15
      v5 = v5 xor 10
      v5 = (v5 shl (32-7)) or (v5 shr 7)

    # Round 3.
    block rnd3:
      v0 += m[11]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v1 += m[12]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v2 += m[5]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v3 += m[15]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v2 += m[2]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v3 += m[13]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v1 += m[0]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-7)) or (v5 shr 7)
      v0 += m[8]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v0 += m[10]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v1 += m[3]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v2 += m[7]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v3 += m[9]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v2 += m[1]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v3 += m[4]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v1 += m[6]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v0 += m[14]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-7)) or (v5 shr 7)

    # Round 4.
    block rnd4:
      v0 += m[7]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v1 += m[3]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v2 += m[13]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v3 += m[11]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v2 += m[12]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v3 += m[14]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v1 += m[1]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-7)) or (v5 shr 7)
      v0 += m[9]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v0 += m[2]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v1 += m[5]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v2 += m[4]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v3 += m[15]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v2 += m[0]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v3 += m[8]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v1 += m[10]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v0 += m[6]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-7)) or (v5 shr 7)

    # Round 5.
    block rnd5:
      v0 += m[9]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v1 += m[5]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v2 += m[2]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v3 += m[10]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v2 += m[4]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v3 += m[15]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v1 += m[7]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-7)) or (v5 shr 7)
      v0 += m[0]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v0 += m[14]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v1 += m[11]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v2 += m[6]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v3 += m[3]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v2 += m[8]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v3 += m[13]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v1 += m[12]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v0 += m[1]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-7)) or (v5 shr 7)

    # Round 6.
    block rnd6:
      v0 += m[2]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v1 += m[6]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v2 += m[0]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v3 += m[8]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v2 += m[11]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v3 += m[3]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v1 += m[10]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-7)) or (v5 shr 7)
      v0 += m[12]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v0 += m[4]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v1 += m[7]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v2 += m[15]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v3 += m[1]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v2 += m[14]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v3 += m[9]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v1 += m[5]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v0 += m[13]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-7)) or (v5 shr 7)

    # Round 7.
    block rnd7:
      v0 += m[12]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v1 += m[1]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v2 += m[14]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v3 += m[4]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v2 += m[13]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v3 += m[10]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v1 += m[15]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-7)) or (v5 shr 7)
      v0 += m[5]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v0 += m[0]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v1 += m[6]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v2 += m[9]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v3 += m[8]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v2 += m[2]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v3 += m[11]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v1 += m[3]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v0 += m[7]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-7)) or (v5 shr 7)

    # Round 8.
    block rnd8:
      v0 += m[13]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v1 += m[7]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v2 += m[12]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v3 += m[3]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v2 += m[1]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v3 += m[9]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v1 += m[14]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-7)) or (v5 shr 7)
      v0 += m[11]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v0 += m[5]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v1 += m[15]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v2 += m[8]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v3 += m[2]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v2 += m[6]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v3 += m[10]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v1 += m[4]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v0 += m[0]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-7)) or (v5 shr 7)

    # Round 9.
    block rnd9:
      v0 += m[6]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v1 += m[14]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v2 += m[11]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v3 += m[0]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v2 += m[3]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v3 += m[8]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v1 += m[9]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-7)) or (v5 shr 7)
      v0 += m[15]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v0 += m[12]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v1 += m[13]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v2 += m[1]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v3 += m[10]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v2 += m[4]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v3 += m[5]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v1 += m[7]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v0 += m[2]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-7)) or (v5 shr 7)

    # Round 10.
    block rnd10:
      v0 += m[10]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v1 += m[8]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v2 += m[7]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v3 += m[1]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v2 += m[6]
      v2 += v6
      v14 = v14 xor v2
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v10 += v14
      v6 = v6 xor v10
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v3 += m[5]
      v3 += v7
      v15 = v15 xor v3
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v11 += v15
      v7 = v7 xor v11
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v1 += m[4]
      v1 += v5
      v13 = v13 xor v1
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v9 += v13
      v5 = v5 xor v9
      v5 = (v5 shl (32-7)) or (v5 shr 7)
      v0 += m[2]
      v0 += v4
      v12 = v12 xor v0
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v8 += v12
      v4 = v4 xor v8
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v0 += m[15]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-16)) or (v15 shr 16)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-12)) or (v5 shr 12)
      v1 += m[9]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-16)) or (v12 shr 16)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-12)) or (v6 shr 12)
      v2 += m[3]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-16)) or (v13 shr 16)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-12)) or (v7 shr 12)
      v3 += m[13]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-16)) or (v14 shr 16)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-12)) or (v4 shr 12)
      v2 += m[12]
      v2 += v7
      v13 = v13 xor v2
      v13 = (v13 shl (32-8)) or (v13 shr 8)
      v8 += v13
      v7 = v7 xor v8
      v7 = (v7 shl (32-7)) or (v7 shr 7)
      v3 += m[0]
      v3 += v4
      v14 = v14 xor v3
      v14 = (v14 shl (32-8)) or (v14 shr 8)
      v9 += v14
      v4 = v4 xor v9
      v4 = (v4 shl (32-7)) or (v4 shr 7)
      v1 += m[14]
      v1 += v6
      v12 = v12 xor v1
      v12 = (v12 shl (32-8)) or (v12 shr 8)
      v11 += v12
      v6 = v6 xor v11
      v6 = (v6 shl (32-7)) or (v6 shr 7)
      v0 += m[11]
      v0 += v5
      v15 = v15 xor v0
      v15 = (v15 shl (32-8)) or (v15 shr 8)
      v10 += v15
      v5 = v5 xor v10
      v5 = (v5 shl (32-7)) or (v5 shr 7)

    h0 = h0 xor (v0 xor v8)
    h1 = h1 xor (v1 xor v9)
    h2 = h2 xor (v2 xor v10)
    h3 = h3 xor (v3 xor v11)
    h4 = h4 xor (v4 xor v12)
    h5 = h5 xor (v5 xor v13)
    h6 = h6 xor (v6 xor v14)
    h7 = h7 xor (v7 xor v15)

    p = p[BlockSize..^1]
    #echo "d.h before: ", (@(d.h))
    d.h[0] = h0; d.h[1] = h1; d.h[2] = h2; d.h[3] = h3
    d.h[4] = h4; d.h[5] = h5; d.h[6] = h6; d.h[7] = h7
    #echo "d.h after: ", (@(d.h))